# jemdoc: menu{MENU}{index.html}, addcss{examples.css}, addcss{mktree.css}, nofooter
= Liu Liu

~~~
{}{img_left}{./photo-me.jpg}{photo}{292px}{320px}{}

I am currently a Senior Research Scientist at Tencent AI Lab, where I specialize in leveraging advanced AI technologies to enhance the design and performance of decision intelligence systems for real-world applications. 
My recent work primarily involves advanced algorithms in reinforcement learning and operations research, with applications to smart city. 

I received PhD in the Department of Electrical and Computer Engineering of The University of Texas at Austin in 2021,
and I was fortunately advised by [https://caramanis.github.io/ Prof. Constantine Caramanis], IEEE Fellow.
During my PhD, I worked on robust machine learning models and algorithms
for high-dimensional data analysis.

I received the B.E. degree in the Department of Electronic Engineering of Tsinghua University in 2016.

~~~




~~~

== Experience

- 2021-present: *Tencent AI Lab*, Senior Research Scientist \n
- 2016-2021: *The University of Texas at Austin*, PhD candidate \n
- 2012-2016: *Tsinghua University*, Undergraduate \n
~~~

~~~

== Contact

- Email: leon_liuliu \[at\] outlook \[dot\] com \n
- [https://scholar.google.com/citations?user=hP8aSTAAAAAJ&hl=en Google Scholar Profile]
~~~


~~~
{}{raw}
<a name="research">
~~~

== Selected Publications
~~~
{}{raw}
<script type="text/javascript" language="JavaScript" src="mktree.js"></script>
<p class="jshide" id="jsonly"> 
<a href="#" onclick="expandTree('tree1'); return false;">Expand all abstracts</a>&nbsp;&nbsp;
<a href="#" onclick="collapseTree('tree1'); return false;">Collapse all abstracts</a></p>
~~~


{{<div class="mktree" id="tree1">}}





*Rethinking Safe Policy Learning for Complex
Constraints Satisfaction: A Glimpse in Real-Time
Security Constrained Economic Dispatch Integrating
Energy Storage Units*\n
Jianxiong Hu, Yujian Ye, Yizhi Wu, Peilin Zhao, *Liu Liu*. \n
IEEE Transactions on Power Systems. To appear

\n


*Pareto Deep Long-Tailed Recognition: A Conflict-Averse Solution *\n
Zhipeng Zhou, *Liu Liu**, Peilin Zhao, Wei Gong.\n
ICLR 2024. \[[https://openreview.net/forum?id=b66P1u0k15 Link]\]
- *See abstract*
-- Deep long-tailed recognition (DTLR) has attracted much attention due to its close touch with realistic scenarios. Recent advances have focused on re-balancing across various aspects, e.g., sampling strategy, loss re-weighting, logit adjustment, and input/parameter perturbation, to name a few. However, few studies have considered dynamic re-balancing to address intrinsic optimization conflicts. In this paper, we first empirically argue that the optimizations of mainstream DLTR methods are still dominated by some categories (e.g., major) due to a fixed re-balancing strategy. Thus, they fail to deal with gradient conflicts among categories, which naturally deduces the motivation for reaching Pareto optimal solutions. Unfortunately, a naive integration of multi-objective optimization (MOO) with DLTR methods is not applicable due to the gap between multi-task learning (MTL) and DLTR, and can in turn lead to class-specific feature degradation. Thus, we provide effective alternatives by decoupling MOO-based MTL from the temporal rather than structure perspective, and enhancing it via optimizing variability collapse loss motivated by the derived MOO-based DLTR generalization bound. Moreover, we resort to anticipating worst-case optimization with theoretical insights to further ensure convergence. We build a Pareto deep long-tailed recognition method termed PLOT upon the proposed MOO framework. Extensive evaluations demonstrate that our method not only generally improves mainstream pipelines, but also achieves an augmented version to realize state-of-the-art performance across multiple benchmarks.

\n

*Dynamics Adapted Imitation Learning *\n
Zixuan Liu, *Liu Liu**, Bingzhe Wu, Lanqing Li, Xueqian Wang, Bo Yuan, Peilin Zhao.\n
Transactions on Machine Learning Research 2023. \[[https://openreview.net/forum?id=w36pqfaJ4t Link]\]
- *See abstract*
-- We consider Imitation Learning with dynamics variation between the expert demonstration (source domain) and the environment (target domain). Based on the popular framework of Adversarial Imitation Learning, we propose a novel algorithm â€“ Dynamics Adapted Imitation Learning (DYNAIL), which incorporates the dynamics variation into the state-action occupancy measure matching as a regularization term. The dynamics variation is modeled by a pair of classifiers to distinguish between source dynamics and target dynamics. Theoretically, we provide an upper bound on the divergence between the learned policy and expert demonstrations in the source domain. Our error bound only depends on the expectation of the discrepancy between the source and target dynamics for the optimal policy in the target domain. The experiment evaluation validates that our method achieves superior results on high dimensional continuous control tasks, compared to existing imitation learning methods.

\n

*BEEF: Bi-Compatible Class-Incremental Learning via Energy-Based Expansion and Fusion *\n
Fu-Yun Wang, Da-Wei Zhou, *Liu Liu**, Han-Jia Ye, Yatao Bian, De-Chuan Zhan, Peilin Zhao.\n
ICLR 2023. \[[https://openreview.net/forum?id=iP77_axu0h3 Link]\]
- *See abstract*
-- Neural networks suffer from catastrophic forgetting when sequentially learning tasks phase-by-phase, making them inapplicable in dynamically updated systems. Class-incremental learning (CIL) aims to enable neural networks to learn different categories at multi-stages. Recently, dynamic-structure-based CIL methods achieve remarkable performance. However, these methods train all modules in a coupled manner and do not consider possible conflicts among modules, resulting in spoilage of eventual predictions. In this work, we propose a unifying energy-based theory and framework called Bi-Compatible Energy-Based Expansion and Fusion (BEEF) to analyze and achieve the goal of CIL. We demonstrate the possibility of training independent modules in a decoupled manner while achieving bi-directional compatibility among modules through two additionally allocated prototypes, and then integrating them into a unifying classifier with minimal cost. Furthermore, BEEF extends the exemplar-set to a more challenging setting, where exemplars are randomly selected and imbalanced, and maintains its performance when prior methods fail dramatically.
Extensive experiments on three widely used benchmarks: CIFAR-100, ImageNet-100, and ImageNet-1000 demonstrate that BEEF achieves state-of-the-art performance in both the ordinary and challenging  CIL settings.

\n




*Robust compressed sensing of generative models *\n
Ajil Jalal, *Liu Liu*, Alexandros G Dimakis, Constantine Caramanis.\n
NeurIPS 2020. \[[https://arxiv.org/abs/2006.09461 Arxiv Link]\]
- *See abstract*
-- The goal of compressed sensing is to estimate a high dimensional vector from an underdetermined system of noisy linear equations. In analogy to classical compressed sensing, here we assume a generative model as a prior, that is, we assume the vector is represented by a deep generative model G: R^k --> R^n. Classical recovery approaches such as empirical risk minimization (ERM) are guaranteed to succeed when the measurement matrix is sub-Gaussian. However, when the measurement matrix and measurements are heavy-tailed or have outliers, recovery may fail dramatically. In this paper we propose an algorithm inspired by the Median-of-Means (MOM). Our algorithm guarantees recovery for heavy-tailed data, even in the presence of outliers. Theoretically, our results show our novel MOM-based algorithm enjoys the same sample complexity guarantees as ERM under sub-Gaussian assumptions. Our experiments validate both aspects of our claims: other algorithms are indeed fragile and fail under heavy-tailed and/or corrupted data, while our approach exhibits the predicted robustness.

\n


*Robust Structured Statistical Estimation via Conditional Gradient Type Methods*\n
Jiacheng Zhuo, *Liu Liu*, Constantine Caramanis.\n
arXiv preprint arXiv:2007.03572, 2020. \[[https://arxiv.org/abs/2007.03572 Arxiv Link]\] 
- *See abstract*
-- Structured statistical estimation problems are often solved by Conditional Gradient (CG) type methods to avoid the computationally expensive projection operation. However, the existing CG type methods are not robust to data corruption. To address this, we propose to robustify CG type methods against Huber's corruption model and heavy-tailed data. First, we show that the two Pairwise CG methods are stable, ie, do not accumulate error. Combined with robust mean gradient estimation techniques, we can therefore guarantee robustness to a wide class of problems, but now in a projection-free algorithmic framework. Next, we consider high dimensional problems. Robust mean estimation based approaches may have an unacceptably high sample complexity. When the constraint set is a \ell_0  norm ball, Iterative-Hard-Thresholding-based methods have been developed recently. Yet extension is non-trivial even for general sets with O(d)  extreme points. For setting where the feasible set has O(poly(d)) extreme points, we develop a novel robustness method, based on a new condition we call the Robust Atom Selection Condition (RASC). When RASC is satisfied, our method converges linearly with a corresponding statistical error, with sample complexity that scales correctly in the sparsity of the problem, rather than the ambient dimension as would be required by any approach based on robust mean estimation.

\n


*High Dimensional Robust M-Estimation: Arbitrary Corruption and Heavy Tails *\n
*Liu Liu*, Tianyang Li, Constantine Caramanis.\n
ArXiv preprint arXiv:1901.08237, 2019. \[[http://arxiv.org/abs/1901.08237 Arxiv Link]\] \[[./Oral_Defense_Slides.pdf Slides]\]
- *See abstract*
-- We consider the problem of sparsity-constrained $M$-estimation when both {\em explanatory and response} variables have heavy tails (bounded 4-th moments), or a fraction of arbitrary corruptions. We focus on the $k$-sparse, high-dimensional regime where the number of variables $d$ and the sample size $n$ are related through $n \sim k \log d$. We define a natural condition we call the Robust Descent Condition (RDC), and show that if a gradient estimator satisfies the RDC, then Robust Hard Thresholding (IHT using this gradient estimator), is guaranteed to obtain good statistical rates. The contribution of this paper is in showing that this RDC is a flexible enough concept to recover known results, and obtain new robustness results. Specifically, new results include: (a) For $k$-sparse high-dimensional linear- and logistic-regression with heavy tail (bounded 4-th moment) explanatory and response variables, a linear-time-computable median-of-means gradient estimator satisfies the RDC, and hence Robust Hard Thresholding is minimax optimal; (b) When instead of heavy tails we have $O(1/\sqrt{k}\log(nd))$-fraction of arbitrary corruptions in explanatory and response variables, a near linear-time computable trimmed gradient estimator satisfies the RDC, and hence Robust Hard Thresholding is minimax optimal.
We demonstrate the effectiveness of our approach in sparse linear, logistic regression, and sparse precision matrix estimation on synthetic and real-world US equities data.

\n

*High Dimensional Robust Sparse Regression*\n
*Liu Liu*, Yanyao Shen, Tianyang Li, Constantine Caramanis.\n
AISTATS 2020. \[[https://arxiv.org/abs/1805.11643  Arxiv Link]\] \[[./AISTATS_Slides.pdf Slides]\]
- *See abstract*
-- We provide a novel -- and to the best of our knowledge, the first -- algorithm for high dimensional sparse regression with constant fraction of corruptions in explanatory and/or response variables. Our algorithm recovers the true sparse parameters with sub-linear sample complexity, in the presence of a constant fraction of arbitrary corruptions. Our main contribution is a robust variant of Iterative Hard Thresholding. Using this, we provide accurate estimators: when the covariance matrix in sparse regression is identity,  our error guarantee is near information-theoretically optimal. We then deal with robust sparse regression with unknown structured covariance matrix. We propose a filtering algorithm which consists of a novel randomized outlier removal technique for robust sparse mean estimation that may be of interest in its own right: the filtering algorithm is flexible enough to deal with unknown covariance. Also, it is orderwise more efficient computationally than the ellipsoid algorithm. Using sub-linear sample complexity, our algorithm achieves the best known (and first) error guarantee. 
We demonstrate the effectiveness on large-scale sparse regression problems with arbitrary corruptions.

\n

*Statistical inference using SGD*\n
Tianyang Li, *Liu Liu*, Anastasios Kyrillidis, Constantine Caramanis.\n
AAAI 2018. \[[https://arxiv.org/abs/1705.07477  Arxiv Link]\] 
- *See abstract*
-- We present a novel method for frequentist statistical inference in M-estimation problems, based on stochastic gradient descent (SGD) with a fixed step size: we demonstrate that the average of such SGD sequences can be used for statistical inference, after proper scaling. An intuitive analysis using the Ornstein-Uhlenbeck process suggests that such averages are asymptotically normal. From a practical perspective, our SGD-based inference procedure is a first order method, and is well-suited for large scale problems. To show its merits, we apply it to both synthetic and real datasets, and demonstrate that its accuracy is comparable to classical statistical methods, while requiring potentially far less computation.

\n


*Approximate Newton-based statistical inference using only stochastic gradients*\n
Tianyang Li, Anastasios Kyrillidis, *Liu Liu*, Constantine Caramanis.\n
ArXiv preprint arXiv:1805.08920, 2018. \[[https://arxiv.org/abs/1805.08920  Arxiv Link]\] 
- *See abstract*
-- We present a novel inference framework for convex empirical risk minimization, using approximate stochastic Newton steps. The proposed algorithm is based on the notion of finite differences and allows the approximation of a Hessian-vector product from first-order information. In theory, our method efficiently computes the statistical error covariance in M-estimation, both for unregularized convex learning problems and high-dimensional LASSO regression, without using exact second order information, or resampling the entire data set. In practice, we demonstrate the effectiveness of our framework on large-scale machine learning problems, that go even beyond convexity: as a highlight, our work can be used to detect certain adversarial attacks on neural networks.


\n

\n
\n

